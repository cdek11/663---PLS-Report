{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\documentclass[fleqn]{article}\n",
    "\\title{Beta-Binomial Model}\n",
    "\\author{Jonathan Bryan}\n",
    "\\date{March 12 2016}\n",
    "\\usepackage[fleqn]{amsmath}\n",
    "\\usepackage{amssymb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Partial Least Squares (PLS) </center>\n",
    "### <center> Jonathan Bryan and Carmen Dekmezian </center>\n",
    "\n",
    "\n",
    "### Abstract\n",
    "\n",
    "### Background\n",
    "The paper we selected is titled “Partial Least Squares Regression: A Tutorial” by Geladi and Kowalski. It discusses partial least squares regression (PLSR), which is a method for decomposing values of predictors and response into more basic components and finding a robust linear relationship between the two components. PLSR was formulated by econometrician Herman Wold as a technique for \"soft modelling\", when the number of variables is high, the relationship between the variables is poorly understood and prediction is the primary goal. His son Svante Wold extended the use of PLSR to chemometrics to model high dimensional chemical data.\n",
    "\n",
    "The nonexistence of a unique solution when the number of predictors is larger than the sample size of the data is a challenging problem in multiple linear regression, especially when multicollinearity and missing are also issues. Other methods that seek to reduce the dimensionality of the predictors, such as principal component analysis (PCR), allow for noise reduction and solve the collinearity problem. However, useful information for accurate prediction can be lost because PCR explains the useful directional information in the predictor space which may not be sufficiently linked to the space of new observed responses\n",
    "\n",
    "PLSR produces X-scores, much like PCR, but also Y-scores that explain the given response space. These scores are generated by seeking important directions in the X-scores that are strongly associated with variation in the Y-scores and biased towards accurate predictions.\n",
    "\n",
    "PLSR has several advantages over other methods. It is advantageous when the number of predictors is larger than the sample size and collinearity is high among the predictors. It is also a robust method regression because it reduces out of sample variance of residual errors and noise in the data in comparison to common multiple least squares regression algorithms. Additionally, it is beneficial when there are missing observations in the data. \n",
    "\n",
    "One disadvantage of PLSR is that the use of lower dimensional representations of the data and loadings of the model can make the interpretability of the important latent predictors difficult in some situations.\n",
    "\n",
    "### Description of algorithm\n",
    "\n",
    "PLS is a regression method used to overcome limitations discussed above for normal linear regressions (e.g., many collinear predictors, more predictors than samples, etc.) by mapping observed sets of observed variables to response variables by means of latent variables. Essentially the model assumes that the data is generated by an underlying model directed by a smaller number of latent variables in the data. \n",
    "\n",
    "First, two sets of latent variables are extracted from the data: $T$ (or x-scores) from the predictors, and $U$ (or y-scores) from the response variable. These latent vectors are determined through maximizing the covariance between different sets of variables. \n",
    "\n",
    "For the classic linear regression, we try to solve the equation, $ Y = X\\beta + {\\epsilon} $, where the ordinary least squares estimate for ${\\beta}$ is identified as ${(X^T X)}^{-1} X^TY$. This estimate is obtained by minimizing the sum of squared residuals. However, models that have predictors with high collinearity or more predictors than observations can result in singularity of the matrix ${(X^T X)}$. As an alternative and way to fix this issue, we implement the PLS algorithm throught the following steps:\n",
    "\n",
    "1) Start with vector $u$. If there is only one response variable, then $ u = y $, otherwise it is one of the columns of $Y$.\n",
    "\n",
    "2) Calculate the weights for the predictors ($X$) :\n",
    "$$ w = \\frac{X^Tu}{u^Tu} $$\n",
    "\n",
    "3) Determine $t$ ($X$ scores):\n",
    "\n",
    "$$ t = Xw $$\n",
    "\n",
    "4) Now perform similar calculations for $Y$. Calculate the weights for the response variable:\n",
    "$$ c = \\frac{X^Tt}{t^Tt} $$\n",
    "\n",
    "5) Determine $u$ ($Y$ scores):\n",
    "$$ u = \\frac{Yc}{c^Tc} $$\n",
    "\n",
    "6) If there is more than one response variable, then we test to determine whether the $t$ values have converged. If the change in $t$ from one iteration to the next, $ \\frac{||t_{old} - t_{new}}{||t_{new}||} $, is not smaller than a threshold value, then we iterate through steps 2-5 until convergence is reached.\n",
    "\n",
    "7) Deflate variables for next iteration.\n",
    "$$ p = \\frac{X^Tt}{t^Tt} $$\n",
    "$$ X = X - tp^T $$\n",
    "$$ Y = Y - tc^T $$\n",
    "\n",
    "8) Iterate through components until they are not found to be predictive of $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "%matplotlib inline\n",
    "import time\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import cross_validation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression as lin_reg\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in ames home price train and test data\n",
    "ames_train = pd.read_csv(\"ames_train.csv\")\n",
    "ames_test = pd.read_csv(\"ames_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge both files first so there are the same number of categories created\n",
    "ames_merged = pd.concat([ames_train, ames_test])\n",
    "ames_merged = pd.get_dummies(ames_merged).drop([\"price\", \"PID\"], axis = 1).fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert categorical variables to dummy variables\n",
    "# create variables for train and test data separately, x for predictors, y for response\n",
    "\n",
    "x_predictors_train = ames_merged[0:1500]\n",
    "x_predictors_test = ames_merged[1500:2000]\n",
    "\n",
    "y_train = ames_train.price\n",
    "y_test = ames_test.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Partial Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pls(path, predictors, response):\n",
    "    '''Function that takes a dataframe and runs partial least squares on numeric predictors for a numeric response.\n",
    "    Returns the residuals of the predictor (X block), response (Y block), and traininig RMSE'''\n",
    "    combined = predictors\n",
    "    #Load data\n",
    "    data = pd.DataFrame.from_csv(path)\n",
    "    combined.append(response)\n",
    "    data = data[combined]\n",
    "    response_std = data[response].std()\n",
    "    \n",
    "    #Subtract the mean from each column\n",
    "    data = data - data.mean()\n",
    "\n",
    "    #Scale each column by the column standard deviation\n",
    "    data = data/data.std()\n",
    "\n",
    "    #Separate in to design matrix (X block) and response column vector (Y block)\n",
    "    X = data[predictors].as_matrix()\n",
    "    Y = data[[response]].as_matrix()\n",
    "    \n",
    "    #SEE PAGE 16\n",
    "\n",
    "    #Here we have one variable in the Y block so q = 1 \n",
    "    #and omit steps 5-8\n",
    "    q = 1\n",
    "\n",
    "    #For the X block, u = Y\n",
    "    u = Y #random y column from Y #Step 1\n",
    "    w_old = np.dot(u.T,X)/np.dot(u.T,u) #Step 2\n",
    "    w_new = w_old/np.linalg.norm(w_old) #Step 3\n",
    "    t = np.dot(X,w_new.T)/np.dot(w_new,w_new.T) #Step 4\n",
    "\n",
    "    #For the Y block can be omitted if Y only has one variable\n",
    "    q_old = np.dot(t.T,Y)/np.dot(t.T,t) #Step 5\n",
    "    q_new = q_old/np.linalg.norm(q_old) #Step 6\n",
    "    u = np.dot(Y,q_new.T)/np.dot(q_new,q_new.T) #Step 7\n",
    "\n",
    "    #Step 8: Check convergence\n",
    "\n",
    "    #Calculate the X loadings and rescale the scores and weights accordingly\n",
    "    p = np.dot(t.T,X)/np.dot(t.T,t) #Step 9\n",
    "    p_new = p.T/np.linalg.norm(p.T) #Step 10\n",
    "    t_new = t/np.linalg.norm(p.T) #Step 11\n",
    "    w_new = w_old/np.linalg.norm(p)  #Step 12\n",
    "\n",
    "    #Find the regression coefficient for b for th inner relation\n",
    "    b = np.dot(u.T,t_new)/np.dot(t.T,t) #Step 13\n",
    "    b\n",
    "\n",
    "    #Calculation of the residuals\n",
    "    E_h = X - np.dot(t_new,p_new.T)\n",
    "    F_h = Y - b.dot(t_new.T).T.dot(q) #WORKS BUT IS THIS RIGHT?\n",
    "\n",
    "    RMSE = np.sqrt(sum((F_h)**2)/F_h.shape[0]) \n",
    "    RMSE_scaled = RMSE * response_std # I believe this is the RMSE since the Y had to be scaled.\n",
    "    \n",
    "    return E_h,F_h,RMSE_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.76742694,  0.64842113, -0.4663534 , ..., -0.81244013,\n",
       "         -0.52528266, -0.27248012],\n",
       "        [ 1.32210502,  0.68291909, -0.53425924, ..., -1.94333352,\n",
       "         -0.31297512,  0.04158601],\n",
       "        [ 2.03171353, -0.1268066 ,  0.78350473, ...,  0.26685303,\n",
       "         -0.87120918, -0.49391333],\n",
       "        ..., \n",
       "        [ 0.79769126, -0.15036518,  0.19601739, ..., -0.77724249,\n",
       "          0.2607463 ,  0.05086685],\n",
       "        [ 0.89329861,  0.16446538,  0.103049  , ..., -0.66605039,\n",
       "         -0.35958269, -0.0183738 ],\n",
       "        [ 0.72425218,  0.23925765,  0.04681011, ..., -0.86265271,\n",
       "          0.16406964, -0.30358697]]), array([[-0.27248012],\n",
       "        [ 0.04158601],\n",
       "        [-0.49391333],\n",
       "        ..., \n",
       "        [ 0.05086685],\n",
       "        [-0.0183738 ],\n",
       "        [-0.30358697]]), array([ 27401.91445331]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'ames_train.csv'\n",
    "predictors = [\"Fireplaces\", \"TotalSq\", \"Lot.Area\", \"area\",\"Garage.Cars\",\"Overall.Qual\"]\n",
    "response = \"price\"\n",
    "pls(path, predictors, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications to real data sets\n",
    "\n",
    "In the paper we selected, the authors don’t include real-world examples with data that we can test on. Thus, we decided to test the algorithm on a publicly available dataset that we investigated for our predictive modeling class. This data contains information on residential properties in Ames, Iowa, with 79 variables that describe characteristics of each home and a variable indicating the home’s value.[ref: Decook]\n",
    "\n",
    "We decided to use this dataset because it contains many correlated variables, some of which have very similar meanings. For example, the dataset includes the variables Garage.qual and Garage.cond, both of which can take on values of: Ex (excellent), Gd (good), TA (typical/average), Fa (fair), Po (Poor), or NA (no garage). Additionally, since there are so many different factor/categorical variables in the dataset, the total number of predictors becomes 288 when trying to estimate a coefficient for each level. Since these characteristics play into the strength of the PLS method, PLS method is beneficial \n",
    "\n",
    "With this data, we were trying to predict a home’s price based on its attributes. The limitations of the data, including the number of predictors and collinearity, suggest that the PLS method will be beneficial for predictions and accuracy.\n",
    "\n",
    "After running the PLS method using the training data and performing cross-validations, we then tested our model on the test data. Our final model ended up giving us a root mean squared error of ____________."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative analysis with competing algorithms\n",
    "\n",
    "With the Ames housing data, we decided to test our PLS results above with the standard OLS regression and the PCR method. In the OLS regression, we tried predicting price using all of the variables in the dataset. The model is as follows:\n",
    "$$ y = X\\beta + \\epsilon $$\n",
    "$$ y = \\beta_0 + x_1\\beta_1 + ... + x_p\\beta_p + \\epsilon $$ \n",
    "$$ y = \\beta_0 + area^*\\beta_{area} + ... + TotalSq^*\\beta_{TotalSq} + \\epsilon $$ \n",
    "where we have $p$ predictors available in the data.\n",
    "\n",
    "For the OLS model, $\\beta$ is estimated by minimizing the sum of squared errors and can be express as ${(X^T X)}^{-1} X^TY$. All predictors are used in estimating the $\\beta$ coefficients. The PCR regression is similar to the PLS regression in that it selects a subset of features out of the larger number of predictors to include in the model. In the PCR model, the predictors are first scaled and then we perform 10-fold cross-validation on the model with varying numbers of predictors (from 0, which is a model with just the intercept, up to the total number of predictors). We find the model with the smallest RMSE to identify the number of principal components to be used in the final model (see the plot below for a visualization). Using that set of predictors, we predict the house price on the test data and calculate the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Algorithm: OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# speed of algorithm\n",
    "ols_start = timeit.default_timer()\n",
    "\n",
    "# run ols on home price model\n",
    "ols_reg = lin_reg().fit(x_predictors_train, y_train)\n",
    "y_test_pred_ols = ols_reg.predict(x_predictors_test)\n",
    "ols_rmse_test = sqrt(mean_squared_error(y_test, y_test_pred_ols))\n",
    "\n",
    "ols_speed = timeit.default_timer() - ols_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Algorithm: PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# speed of pcr algorithm\n",
    "pcr_start = timeit.default_timer()\n",
    "\n",
    "# scale x train data\n",
    "x_scaled_train = PCA().fit_transform(scale(x_predictors_train))\n",
    "n = len(x_scaled_train)\n",
    "\n",
    "# perform cross validation (10 k-fold)\n",
    "cross_val_k_10 = cross_validation.KFold(n, n_folds=10, shuffle=True, random_state=1)\n",
    "pcr_rmse = []\n",
    "\n",
    "# calculate rmse values for intercept (no predictors/components)\n",
    "mean_sq_error = -1*cross_validation.cross_val_score(lin_reg(), np.ones((n,1)), y_train.ravel(), cv = cross_val_k_10, scoring = \"neg_mean_squared_error\").mean()\n",
    "pcr_rmse.append(sqrt(mean_sq_error))\n",
    "\n",
    "# calculate rmse values for varying numbers of predictors/components\n",
    "for i in range(1, len(x_predictors_train.columns)):\n",
    "    mean_sq_error = -1*cross_validation.cross_val_score(lin_reg(), x_scaled_train[:,:i], y_train.ravel(), cv = cross_val_k_10, scoring = \"neg_mean_squared_error\").mean()\n",
    "    pcr_rmse.append(sqrt(mean_sq_error))\n",
    "\n",
    "# find number of principal components with smallest rmse\n",
    "n_pcr_comps = pcr_rmse.index(min(pcr_rmse))\n",
    "\n",
    "# run regression using specific number of principal components\n",
    "pcr_reg = lin_reg().fit(x_scaled_train[:,:n_pcr_comps], y_train)\n",
    "\n",
    "# use regression above to predict on test data\n",
    "x_scaled_test = PCA().fit_transform(scale(x_predictors_test))[:,:n_pcr_comps]\n",
    "y_test_pred_pcr = pcr_reg.predict(x_scaled_test)\n",
    "pcr_rmse_test = sqrt(mean_squared_error(y_test, y_test_pred_pcr))\n",
    "    \n",
    "pcr_speed = timeit.default_timer() - pcr_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pcr_rmse)\n",
    "plt.title(\"PCR: RMSE Values by Number of Principal Components\")\n",
    "plt.xlabel(\"# Principal Components in Regression\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.ylim(ymin = 20000, ymax = 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summary of all methods, accuracy and speed\n",
    "diff_methods_summary = {'RMSE' : [1111., pcr_rmse_test, ols_rmse_test], 'Speed' : [1111, pcr_speed, ols_speed]}\n",
    "pd.DataFrame(diff_methods_summary, index=['PLS', 'PCR', 'OLS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion/Conclusion\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "used a lot of it in running pcr \n",
    "http://www.science.smith.edu/~jcrouser/SDS293/labs/lab11/Lab%2011%20-%20PCR%20and%20PLS%20Regression%20in%20Python.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
